# Ecuaci√≥n de Calor en 2D - Paralelizaci√≥n con MPI y OpenMP

Proyecto de paralelizaci√≥n de la ecuaci√≥n de calor en 2D usando diferentes paradigmas de programaci√≥n paralela (OpenMP y MPI).

## üìã Descripci√≥n del Problema

Resoluci√≥n de la ecuaci√≥n de calor bidimensional usando el m√©todo de diferencias finitas con esquema expl√≠cito de Euler. Se simula la transmisi√≥n de calor en una malla de 80√ó80 puntos con condiciones de frontera fijas.

### Ecuaci√≥n

$$\frac{\partial \phi}{\partial t} = \alpha \left(\frac{\partial^2 \phi}{\partial x^2} + \frac{\partial^2 \phi}{\partial y^2}\right)$$

### Par√°metros
- **Malla:** 80√ó80 puntos (configurable)
- **Iteraciones m√°ximas:** 20,000
- **Criterio de convergencia:** $\epsilon = 10^{-8}$
- **Esquema:** Euler expl√≠cito (diferencias finitas)

## üöÄ Versiones Implementadas

### 1. **Secuencial** (`secuencial.cpp`)
- C√≥digo base sin paralelizaci√≥n
- Referencia para validaci√≥n y c√°lculo de speedup
- Tiempo base: ~0.75-1.5s (sin optimizaci√≥n)

### 2. **V1: OpenMP** (`version1.cpp`)
- **Paradigma:** Memoria compartida (multi-threading)
- **Paralelizaci√≥n:** Ambos bucles espaciales con `collapse(2)` (similar a descomposici√≥n 2D)
- **Directivas clave:**
  ```cpp
  #pragma omp parallel for collapse(2) private(dphi) reduction(max:dphimax) schedule(static)
  ```
- **Ventajas:**
  -  Paralelizaci√≥n 2D: divide dominio en grid 2D de iteraciones (79√ó79 = 6,241 iteraciones)
  -  Mejor balanceo de carga con muchos threads
  -  Comparaci√≥n justa con descomposici√≥n 2D de MPI
  -  Sin comunicaci√≥n expl√≠cita
  -  Buen rendimiento en sistemas multi-core

### 3. **V2: MPI No-Bloqueante** (`version2.cpp`)
- **Paradigma:** Memoria distribuida (multi-proceso)
- **Descomposici√≥n:** Dominio 1D por filas
- **Comunicaci√≥n:** 
  - `MPI_Isend` / `MPI_Irecv` para intercambio de halos (ghost rows)
  - `MPI_Allreduce` para convergencia global
- **Estrategia de solapamiento:**
  1. Enviar/recibir halos (no-bloqueante)
  2. Computar puntos interiores (mientras se comunica)
  3. `MPI_Waitall` (esperar comunicaciones)
  4. Computar puntos frontera
- **Ventajas:**
  -  Escalable a m√∫ltiples nodos
  -  Solapamiento comunicaci√≥n/c√≥mputo
- **Overhead:** Comunicaci√≥n aumenta con m√°s procesos (~8-15% en grids peque√±os)

### 4. **V3: Benchmark Completo** (`version3.cpp`)
- **Prop√≥sito:** Herramienta de an√°lisis comparativo
- **Ejecuta:** Secuencial ‚Üí MPI ‚Üí OpenMP (en orden)
- **M√©tricas generadas:**
  - Speedup: $S_p = T_{seq} / T_{par}$
  - Eficiencia: $E_p = (S_p / p) \times 100\%$
  - GFlops (rendimiento computacional)
  - Overhead de comunicaci√≥n (%)
- **Salida:** CSV para an√°lisis (`resultados_benchmark.csv`)
- **Gr√°ficas:** Script Python incluido (`graficar.py`)

## üõ†Ô∏è Compilaci√≥n y Ejecuci√≥n

### **Opci√≥n 1: Usando Makefile (Recomendado)**

```bash
# Compilar todas las versiones
make all

# Compilar versiones individuales
make secuencial
make version1      # OpenMP
make version2      # MPI
make version3      # Benchmark completo

# Ejecutar con configuraciones predefinidas
make run-v1-4      # OpenMP con 4 threads
make run-v2-4      # MPI con 4 procesos
make run-v3-4      # Benchmark con 4 procesos MPI

# Generar datos para gr√°ficas (1, 2, 4 procesos)
make benchmark-csv

# Limpiar ejecutables y datos
make clean

# Ver todas las opciones disponibles
make help
```

### **Opci√≥n 2: Compilaci√≥n Manual**

#### **Secuencial**
```bash
g++ -Wall -o secuencial secuencial.cpp
./secuencial
```

#### **V1: OpenMP**
```bash
g++ -Wall -fopenmp -o version1 version1.cpp

# Ejecutar con 4 threads
export OMP_NUM_THREADS=4
./version1
```

#### **V2: MPI**
```bash
mpic++ -Wall -o version2 version2.cpp

# Ejecutar con 4 procesos
mpirun -np 4 ./version2
```

#### **V3: Benchmark (MPI + OpenMP)**
```bash
# Compilador MPI con flag OpenMP
mpic++ -Wall -fopenmp -o version3 version3.cpp

# Ejecutar con 4 procesos MPI
# (OpenMP usar√° threads disponibles autom√°ticamente)
mpirun -np 4 ./version3
```

## An√°lisis de Resultados

### **Generar Gr√°ficas**

```bash
# 1. Ejecutar benchmarks
make benchmark-csv

# 2. Generar gr√°ficas con Python
python3 graficar.py
```


**Observaciones:**
-  MPI escala mejor que OpenMP en este problema
-  Overhead de comunicaci√≥n crece con m√°s procesos
-  En Khipu con grids grandes (320√ó320+) se espera mejor eficiencia


## Uso en Khipu (Supercomputadora)

Para ejecutar en el cluster Khipu de la universidad:

### **1. Preparar Script SLURM**

```bash
#!/bin/bash
#SBATCH --job-name=calor2d
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=8
#SBATCH --time=00:30:00
#SBATCH --partition=normal

# Compilar con optimizaci√≥n
module load gcc/11.2.0 openmpi/4.1.1
mpic++ -O3 -Wall -fopenmp -o version3 version3.cpp

# Ejecutar benchmark con 32 procesos (4 nodos √ó 8 procesos)
mpirun -np 32 ./version3
```

### **2. Enviar Trabajo**

```bash
sbatch job_calor2d.sh
```

### **3. Escalar Grid para Khipu**

Modificar en los archivos `.cpp`:
```cpp
const int imax = 320;  // Grid m√°s grande
const int kmax = 320;
```

Recomendaciones:
- **80√ó80:** Pruebas locales (1-4 procesos)
- **320√ó320:** Khipu con 16-32 procesos
- **640√ó640:** Khipu con 64-128 procesos

## üî¨ Validaci√≥n de Resultados

Todos los c√≥digos deben converger al mismo n√∫mero de iteraciones:
```
Iteraciones: 14320 (para grid 80√ó80)
```

Verificar que `dphimax < eps` al final de la ejecuci√≥n.


##  Notas de Dise√±o

### **¬øPor qu√© comunicaci√≥n no-bloqueante?**
- Permite solapar comunicaci√≥n con c√≥mputo
- Mejora rendimiento 10-30% vs bloqueante
- Calcula puntos interiores mientras esperan halos

##  Autor

- Manuel Jesus Silva Anampa 
- ...
- ...   

##  Licencia

Proyecto acad√©mico - Universidad
