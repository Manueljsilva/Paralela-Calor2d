# Ecuaci√≥n de Calor en 2D - Paralelizaci√≥n

Proyecto de paralelizaci√≥n de la ecuaci√≥n de calor en 2D usando diferentes paradigmas de programaci√≥n paralela.

## üìã Descripci√≥n del Problema

Resoluci√≥n de la ecuaci√≥n de calor bidimensional usando el m√©todo de diferencias finitas con esquema expl√≠cito de Euler. Se simula la transmisi√≥n de calor en una malla de 80x80 puntos con condiciones de frontera fijas.

## üöÄ Versiones Implementadas

### Cuadro Comparativo de Versiones

| Caracter√≠stica | Secuencial | V1: OpenMP | V2: MPI No-Bloqueante | V3: H√≠brida MPI+OpenMP |
|----------------|------------|------------|----------------------|------------------------|
| **Paradigma** | Secuencial | Memoria Compartida | Memoria Distribuida | Memoria Distribuida + Compartida |
| **Tecnolog√≠a** | C++ est√°ndar | OpenMP | MPI (Isend/Irecv) | MPI + OpenMP |
| **Descomposici√≥n** | - | Ninguna (threads comparten todo) | Dominio 1D (por filas) | Dominio 1D entre procesos, threads dentro |
| **Comunicaci√≥n** | - | Impl√≠cita (memoria compartida) | Expl√≠cita (intercambio de fronteras) | MPI entre procesos, compartida intra-proceso |
| **Tipo de Comunicaci√≥n** | - | - | No-bloqueante (solapamiento) | No-bloqueante entre procesos |
| **Sincronizaci√≥n** | - | Barrera impl√≠cita en directivas | MPI_Wait, MPI_Allreduce | MPI_Wait + barreras OpenMP |
| **Escalabilidad** | - | Limitada (1 nodo, multi-core) | Alta (multi-nodo) | Muy Alta (multi-nodo + multi-core) |
| **Complejidad Implementaci√≥n** | Baja | Baja | Media-Alta | Alta |
| **Balanceo de Carga** | - | Autom√°tico (schedule) | Manual (distribuci√≥n filas) | H√≠brido |
| **Regiones Paralelizadas** | - | Bucles espaciales (k) | Descomposici√≥n de dominio por filas | Ambos niveles |
| **Variables Cr√≠ticas** | - | `dphimax` (reduction) | `dphimax` (MPI_Allreduce) | Reducci√≥n en ambos niveles |
| **Comunicaci√≥n/C√≥mputo** | - | - | Solapado (Isend/Irecv) | Solapado multi-nivel |
| **Ideal para** | Validaci√≥n | Sistemas multi-core | Clusters | Supercomputadoras |

### Detalles de Implementaci√≥n

#### **Versi√≥n Secuencial (Base)**
- C√≥digo original sin modificaciones
- Sirve como referencia para validaci√≥n
- Medici√≥n de tiempo base para calcular speedup

#### **V1: OpenMP (Memoria Compartida)**
**Caracter√≠sticas:**
- Paralelizaci√≥n del bucle externo `k` con `#pragma omp parallel for`
- `reduction(max:dphimax)` para criterio de convergencia
- Variables privadas: `i`, `dphi`
- `schedule(static)` para distribuci√≥n de iteraciones
- No requiere modificaci√≥n de la estructura de datos
- Sincronizaci√≥n autom√°tica en barreras

**Ventajas:**
- ‚úÖ F√°cil implementaci√≥n (pocas l√≠neas de c√≥digo)
- ‚úÖ No requiere comunicaci√≥n expl√≠cita
- ‚úÖ Balanceo autom√°tico de carga
- ‚úÖ Ideal para sistemas multi-core de un solo nodo
- ‚úÖ Buen speedup en arquitecturas modernas

**Limitaciones:**
- ‚ö†Ô∏è Limitado a un solo nodo
- ‚ö†Ô∏è Escalabilidad limitada por cores disponibles

#### **V2: MPI No-Bloqueante (Memoria Distribuida)**
**Caracter√≠sticas:**
- Descomposici√≥n 1D del dominio (por filas)
- Cada proceso calcula subdominio local
- Intercambio de filas frontera (ghost rows) con vecinos
- `MPI_Isend` / `MPI_Irecv` para comunicaci√≥n as√≠ncrona
- `MPI_Allreduce` para `dphimax` global
- Solapamiento de comunicaci√≥n y c√≥mputo

**Estrategia de Comunicaci√≥n:**
1. Iniciar env√≠os/recepciones no-bloqueantes (`MPI_Isend`/`MPI_Irecv`)
2. Calcular puntos interiores mientras se comunican halos (solapamiento)
3. Esperar que terminen comunicaciones (`MPI_Waitall`)
4. Calcular puntos frontera que usan halos
5. Actualizar valores locales
6. Reducci√≥n global de `dphimax` con `MPI_Allreduce`

**Ventajas:**
- ‚úÖ Escalable a m√∫ltiples nodos
- ‚úÖ Mayor rendimiento por solapamiento
- ‚úÖ Distribuci√≥n real de memoria

**Limitaciones:**
- ‚ö†Ô∏è Mayor complejidad de c√≥digo
- ‚ö†Ô∏è Requiere manejo expl√≠cito de fronteras
- ‚ö†Ô∏è Overhead de comunicaci√≥n

#### **V3: H√≠brida MPI + OpenMP**
**Caracter√≠sticas:**
- MPI para distribuci√≥n entre procesos (nodos)
- OpenMP para paralelizar dentro de cada proceso
- Dos niveles de paralelismo
- Reduce comunicaciones MPI vs V2 puro
- Aprovecha arquitectura moderna (multi-nodo + multi-core)

**Estrategia:**
- Nivel externo (MPI): Descomposici√≥n de dominio
- Nivel interno (OpenMP): Paralelizaci√≥n de bucles locales
- Comunicaci√≥n MPI solo entre procesos
- Memoria compartida dentro de cada proceso

**Ventajas:**
- ‚úÖ M√°xima escalabilidad
- ‚úÖ Reduce comunicaciones vs MPI puro
- ‚úÖ Aprovecha todo el hardware disponible
- ‚úÖ Flexible en configuraci√≥n procesos/threads

**Limitaciones:**
- ‚ö†Ô∏è Mayor complejidad
- ‚ö†Ô∏è Requiere ajuste fino de par√°metros
- ‚ö†Ô∏è Debugging m√°s dif√≠cil

## üìä Aspectos Comunes a Todas las Versiones Paralelas

### **Regiones Paralelizadas:**

**OpenMP (V1):**
- Bucle espacial externo `k` con `#pragma omp parallel for`
- Bucle de actualizaci√≥n de valores

**MPI (V2):**
- Descomposici√≥n de dominio: cada proceso calcula su subdominio de filas
- No hay directivas de paralelizaci√≥n, el paralelismo viene de la distribuci√≥n

**H√≠brida (V3):**
- MPI: descomposici√≥n de dominio entre procesos
- OpenMP: paralelizaci√≥n de bucles dentro de cada proceso

### **Regiones NO Paralelizadas:**
- Inicializaci√≥n de condiciones de frontera
- Bucle temporal externo (`it = 1 to itmax`) - **todos los procesos lo ejecutan simult√°neamente**, pero tiene dependencia entre iteraciones

### **Variables Cr√≠ticas:**
- `dphimax`: Requiere reducci√≥n (max) para criterio de convergencia

## üõ†Ô∏è Compilaci√≥n y Ejecuci√≥n

### Usando Makefile (recomendado)

```bash
# Compilar todas las versiones
make all

# Compilar versiones individuales
make secuencial
make version1      # OpenMP
make version2      # MPI

# Ejecutar versiones
make run-secuencial
make run-v1        # OpenMP con 4 threads
make run-v1-2      # OpenMP con 2 threads
make run-v1-4      # OpenMP con 4 threads
make run-v1-8      # OpenMP con 8 threads

make run-v2        # MPI con 4 procesos
make run-v2-2      # MPI con 2 procesos
make run-v2-4      # MPI con 4 procesos
make run-v2-8      # MPI con 8 procesos

# Ejecutar benchmark completo
make benchmark

# Limpiar ejecutables
make clean

# Ver ayuda
make help
```

### Compilaci√≥n manual

```bash
# Secuencial
g++ -O3 -Wall -o secuencial secuencial.cpp
./secuencial

# V1: OpenMP
g++ -O3 -Wall -fopenmp -o version1 version1.cpp
export OMP_NUM_THREADS=4
./version1

# V2: MPI
mpic++ -O3 -Wall -o version2 version2.cpp
mpirun -np 4 ./version2

# V3: H√≠brida (pendiente)
mpic++ -O3 -Wall -fopenmp -o version3 version3.cpp
export OMP_NUM_THREADS=2
mpirun -np 4 ./version3
```

## üìà M√©tricas de Evaluaci√≥n

Para cada versi√≥n se debe medir:
- **Tiempo de ejecuci√≥n**
- **Speedup**: $S_p = \frac{T_{secuencial}}{T_{paralelo}}$
- **Eficiencia**: $E_p = \frac{S_p}{p} \times 100\%$
- **Validaci√≥n**: Comparaci√≥n de resultados num√©ricos

## üìö Referencias

- M√©todo de diferencias finitas para ecuaciones parab√≥licas
- Descomposici√≥n de dominio para problemas 2D
- Comunicaci√≥n no-bloqueante en MPI
- Programaci√≥n h√≠brida MPI+OpenMP
